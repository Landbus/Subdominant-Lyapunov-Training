# -*- coding: utf-8 -*-
"""Lyapunov_Spectra_Final.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1J0i7ROZ8gJfH-EFtqwpBDCS-JW9A4GjO
"""

from __future__ import annotations
import math
import dataclasses
import typing as t
import numpy as numpy
import matplotlib.pyplot as plt

plt.rcParams.update({
    "font.size": 12,
    "axes.labelsize": 12,
    "axes.titlesize": 12,
    "xtick.labelsize": 10,
    "ytick.labelsize": 10,
    "legend.fontsize": 10,
})

Array = numpy.ndarray

#Utility Functions
def orthogonal_matrix_from_qr(P: int, rng: numpy.random.Generator) -> Array:
    """
    Generate a random orthogonal matrix U via QR with positive diag in R.
    Ensures det(U) in {+1, -1}; sign flip is irrelevant for our purposes.
    """
    A = rng.normal(size=(P, P))
    Q, R = numpy.linalg.qr(A)
    # Enforce positive diagonal on R so that Q is unique up to column signs
    s = numpy.sign(numpy.diag(R))
    s[s == 0.0] = 1.0
    Q = Q @ numpy.diag(s)
    return Q

rng = numpy.random.default_rng(0)
U = orthogonal_matrix_from_qr(5, rng)
print(U.shape)
print(numpy.allclose(U.T@U, numpy.eye(5)))

"""$H_{\xi} \in \mathbb{R}^{P \times P}$ is symmetric and positive semi-definite. The defining structural hypothesis of the present class is simultaneous diagonalizability of the family $\{H_{\xi} : \xi \in \Xi\}$: i.e. there exists an orthogonal matrix $U \in \mathbb{R}^{P \times P}$ s.t. $\forall \xi \in \Xi$:
\begin{equation}
    H_{\xi} = U \Lambda U^{\intercal}, \quad \Lambda_{\xi} = \mathrm{diag}(\lambda_{1}(\xi),...,\lambda_{P}(\xi)), \quad \lambda_{i}(\xi) \geq 0
\end{equation}
Equivalently, all $H_{\xi}$ commute pairwise and share the orthonormal eigen-basis given by the columns $u_{1},...,u_{P} \in U$. This hypothesis captures a tractable "commuting curvature" regime in which the stochastic gradient dynamics decouple coordinate-wise in the spectral basis.

Overparameterization is encoded by the flat index set:
\begin{equation}
    \mathcal{Z} := \Big\{ i \in \{1, ...,P\} : \lambda_{i}(\xi) = 0 \text{ for } \mathrm{P}_{\xi}-\text{almost every }\xi \Big\}
\end{equation}
"""

@dataclasses.dataclass
class CommutingQuadraticClass:
    r"""
    Per-sample quadratics with a shared eigenbasis are determined by H_xi.

    Attributes
    ----------
    P : int
        Ambient parameter dimension.
    U : array, shape (P, P)
        Orthogonal eigenbasis shared by all H_xi.
    xi_sampler : callable[[], array]
        Function returning lambda(xi) in R^P for a fresh sample xi (nonnegative entries).
    Z : array[bool], shape (P,), optional
        Boolean mask of flat indices i with lambda_i(xi) == 0 almost surely.
    """
    P: int
    U: Array
    xi_sampler: t.Callable[[], Array]
    Z: t.Optional[Array] = None  # Boolean mask (True = flat / overparameterized)

    # Initialization checks
    def __post_init__(self) -> None:
        # Dimensions
        if self.U.shape != (self.P, self.P):
            raise ValueError(f"U must be shape (P,P) = ({self.P},{self.P}), got {self.U.shape}.")
        # Orthogonality
        if not numpy.allclose(self.U.T @ self.U, numpy.eye(self.P), atol=1e-8):
            raise ValueError("U must be orthogonal (U^T U = I).")
        # Z mask
        if self.Z is not None:
            self.Z = numpy.asarray(self.Z, dtype=bool)
            if self.Z.shape != (self.P,):
                raise ValueError(f"Z must be boolean mask of shape (P,), got {self.Z.shape}.")
        # xi_sampler sanity (optional, deferred-safe)
        if self.xi_sampler is not None:
            try:
                test = self.xi_sampler()
                if not isinstance(test, numpy.ndarray):
                    test = numpy.asarray(test, dtype=float)
                if test.shape != (self.P,):
                    raise ValueError("xi_sampler must return an array of shape (P,).")
            except Exception as e:
                print(f"Warning: xi_sampler test failed ({e}); you may define it later.")

    def sample_lambdas(self) -> Array:
        r"""
        This function samples a single realization of the random curvature spectrum
        \lambda(xi) = [\lambda_{1}(xi),...,\lambda_{P}(\xi)] associated with the
        random Hessian H_{\xi}. It ensures that sampled curvatures are also nonnegative
        and uses Z flat are 0 (see document)
        """
        lambdas = self.xi_sampler()
        #this is the SGD at work
        if not isinstance(lambdas, numpy.ndarray):
            lambdas = numpy.asarray(lambdas, dtype=float)
        if lambdas.shape != (self.P,):
            raise ValueError("xi_sampler must return shape (P,) each call.")
        # Nonnegativity (clip tiny negatives from numeric noise)
        lambdas = numpy.clip(lambdas.astype(float, copy=False), 0.0, numpy.inf)
        # Enforce flats: lambda_i(xi) == 0 for i in Z
        if self.Z is not None:
            lambdas = lambdas.copy()
            lambdas[self.Z] = 0.0
        return lambdas

    def sample_hessian(self) -> Array:
        r"""
        Construct H_xi = U diag(lambda(xi)) U^T, with flats enforced. This
        is random as well becuase H_{\xi} so anything that calls \xi is random
        multiplicatively
        """
        lambdas = self.sample_lambdas()
        return self.U @ numpy.diag(lambdas) @ self.U.T

    # Geometry helpers
    def projectors(self) -> t.Tuple[Array, Array]:
        r"""
        Return (P_parallel, P_normal) for the manifold geometry induced by Z.

        P_parallel = U * Pi_Z * U^T     (tangent to the manifold)
        P_normal   = U * Pi_Zc * U^T    (normal complement)

        Raises if Z is None.
        """
        if self.Z is None:
            raise ValueError("Z mask is not set; projectors undefined.")
        Pi_Z  = numpy.diag(self.Z.astype(float))
        # tangent coordinates
        Pi_Zc = numpy.diag((~self.Z).astype(float))
        # normal coordinates
        P_parallel = self.U @ Pi_Z  @ self.U.T
        P_normal   = self.U @ Pi_Zc @ self.U.T
        return P_parallel, P_normal

    def normal_distance(self, e: Array) -> float:
        """
        Compute ||P_normal * e||, where e = w - w_star is the error vector.
        """
        if e.shape != (self.P,):
            raise ValueError(f"e must be shape (P,), got {e.shape}.")
        if self.Z is None:
            raise ValueError("Z mask is not set; normal distance undefined.")
        _, P_normal = self.projectors()
        return float(numpy.linalg.norm(P_normal @ e))

    # Consistency checks
    def validate_flats(self, trials: int = 100, tol: float = 1e-12) -> None:
        """
        Validate that xi_sampler aligns with Z by sampling a few times.
        (This is a check only; masking is still enforced in sample_lambdas().)
        Raises if sampler returns non-negligible curvature on Z.

        Parameters
        ----------
        trials : int
            Number of samples to test.
        tol : float
            Tolerance above which a lambda on Z is considered nonzero.
        """
        if self.Z is None:
            return  # nothing to validate without Z
        Z = numpy.asarray(self.Z, bool)
        for _ in range(trials):
            lam = self.xi_sampler()
            lam = numpy.asarray(lam, dtype=float)
            if numpy.any(lam[Z] > tol):
                raise ValueError(
                    "xi_sampler produces nonzero curvature on Z; "
                    "mask enforcement will zero it, but sampler "
                    "definition is inconsistent with the theoretical class."
                )

"""Given a step-size sequence $(\eta_{t})_{t \geq 0} \subset (0,\infty)$, the stochastic gradient descent (SGD) update at time $t$ is the mapping:
\begin{equation}
    \Phi_{t} : \mathbb{R}^{P} \to \mathbb{R}^{P}, \quad \Phi_{t}(w) = w - \eta_{t}\nabla \ell_{\xi_{t}}(w)
\end{equation}
Since $\nabla \ell_{\xi}(w) = H_{\xi}(w-w^{*})$, it is natural to study the error iterate $e_{t} := w_{t} - w^{*}$, which evolves according to the linear relation:
\begin{equation}
    e_{t+1} = (I - \eta_{t}H_{\xi_{t}})e_{t}
\end{equation}
we can then write a one-step linear factor which is the linear map that carries a small parameter perturbation forward by one iterate:
\begin{equation}
    J_{t}(w) := D_{w}\Phi_{t}(w) = I - \eta_{t}\nabla^{2}\ell_{\xi_{t}}(w)
\end{equation}
For a small perturbation $\delta w_{t}$ around $w_{t}$, the next perturbation satisfies $\delta w_{t+1} = J_{t}(w_{t})\delta w_{t} + o(||\delta w_{t}||)$. However, in our particular quadratic class defined in~\eqref{h_family}, we have that $\ell_{\xi}(w) = \frac{1}{2}(w-w^{*})^{\intercal}H_{\xi}(w-w^{*})$ so $\nabla^{2}\ell_{\xi}(w) = H_{\xi}$ is constant; which means we have:
\begin{equation}
    J_{t} = I - \eta_{t}H_{\xi_{t}}
\end{equation}
is exact i.e. (no $o(\cdot)$ term). With the shared eigen-basis $U$, this diagonalizes as:
\begin{equation}
    J_{t} = U\,\mathrm{diag}(m_{t,1},...,m_{t,P})U^{\intercal}, \quad m_{t,i} = 1-\eta_{t}\lambda_{i}(\xi_{t})
\end{equation}
this linear factor is the driving force behind why this works. Geometrically, $J_{t}$ is the linearization of $\Phi_{t}$ which describes the dynamics of moving along the parameter space. This means it is the linear map that best explains how nearby points move relative to you. In flat directions (tangent to $\mathcal{M}$), $J_{t}$ provides neutral drift. In curved directions (normal to $\mathcal{M}$), $J_{t}$ damps or amplifies depending on curvature and step size. The product of these linearizations over time is the mechanism by which local geometry aggregates into global stability/instability measurements. Thus, operationally this linear factor encodes stability per direction. In eigen-direction $u_{i}$, the scalar multiplier is $m_{t,i}$. If $|m_{t,i}| < 1$ on average, permutations shrink in that direction (a negative Lyapunov exponent), if $|m_{t,i}| > 1$, they grow (positive Lyapunov exponent).

Introducing spectral coordinates $z_{t} := U^{\intercal}e_{t} \in \mathbb{R}^{P}$, the dynamics decouple into the coordinate-wise scalar recursions:
\begin{equation}
    z_{t+1,i} = m_{t,i}z_{t,i}, \quad (i = 1,...,P; \, t = 0,1,2,...)
\end{equation}
Hence, for each $i$, one has the explicit product representation:
\begin{equation}
    z_{t,i} = \Big(\prod_{s=0}^{t-1}m_{s,i}\Big)z_{0,i}
\end{equation}
Two orthogonal projectors will be used repeatedly. The projector onto the manifold tangent and the projector onto the normal complement respectively are:
\begin{gather}
\mathrm{P}_{||} := U\Pi_{\mathcal{Z}}U^{\intercal}\\
    \mathrm{P}_{||} := U\Pi_{\mathcal{Z}^{c}}U^{\intercal}
\end{gather}
The orthogonal distance from a point $w$ to the manifold $\mathcal{M}$ is then:
\begin{equation}
\begin{aligned}
    \mathrm{dist}_{\perp}(w,\mathcal{M}) & := ||\mathrm{P}_{\perp}(w-w^{*})||\\ & = ||\Pi_{\mathcal{Z}^{c}}U^{\intercal}(w-w^{*})||\\ & = ||z_{\mathcal{Z}^{c}}||
\end{aligned}
\end{equation}
"""

@dataclasses.dataclass
class SGDDynamics:
    """
    Linear SGD dynamics in the commuting quadratic class.
    e_{t+1} = (I - \eta_t H_{xi_t}) e_t, where e_t = w_t - w*.
    Provide either a constant eta or a schedule eta_t = eta_schedule(t).
    """
    model: "CommutingQuadraticClass"
    w_star: Array
    e0: Array
    eta: t.Optional[float] = None
    eta_schedule: t.Optional[t.Callable[[int], float]] = None
    enforce_stability: bool = True
    #whether to enforce eta\lambda<2 automatically
    #will keep log of clipping -> good way to check against theory

    #runtime counters
    clip_count: int = dataclasses.field(init=False, default=0)
    clip_log: t.List[t.Tuple[int, float, float]] = dataclasses.field(init=False, default_factory=list)
    #(step, original eta_t, scaling applied)

    def __post_init__(self) -> None:
        assert self.w_star.shape == (self.model.P,)
        assert self.e0.shape == (self.model.P,)
        if self.eta is None and self.eta_schedule is None:
            raise ValueError("Provide either `eta` or `eta_schedule`.")

    def _eta_t(self, t: int) -> float:
        return float(self.eta if self.eta_schedule is None else self.eta_schedule(t))

    def step(self, t: int, e_t: Array) -> t.Tuple[Array, Array, Array, Array, Array]:
        """One SGD step.
        Returns
        -------
        e_next : Array
            Next error vector e_{t+1}.
        H_t : Array
            Sampled Hessian H_{xi_t}.
        J_t : Array
            Linear factor J_t = I - eta_t H_{xi_t}.
        lambdas_t : Array
            Sampled spectrum lambda(xi_t) in the shared basis.
        m_t : Array
            Scalars m_{t,i} = 1 - eta_t * lambda_i(xi_t).
        """
        eta_t = self._eta_t(t)

        #Pull both lambda(xi_t) and H_{xi_t}; lambda via model.sample_lambdas() ensures Z are flats
        lambdas_t = self.model.sample_lambdas()
        H_t = self.model.U @ numpy.diag(lambdas_t) @ self.model.U.T

        #logarithmic tability safeguard
        max_val = numpy.max(eta_t * lambdas_t)
        if self.enforce_stability and max_val >= 2.0:
          #check
            scale = 1.999 / max_val
            #shrink enough to satisfy window
            eta_orig = eta_t
            eta_t *= scale
            #rescale
            self.clip_count += 1
            self.clip_log.append((t, eta_orig, scale))
            #important returns how many times our stability
            #window actually clipped what we were doing
            print(f"[Clipped] step {t}: η_t rescaled by {scale:.3f} "
                  f"(ηλ_max={numpy.max(eta_t * lambdas_t):.3f})")

        J_t = numpy.eye(self.model.P) - eta_t * H_t
        #constructs linearized jacobian (I-etaH_{xi})
        e_next = J_t @ e_t
        #update
        m_t = 1.0 - eta_t * lambdas_t
        #computes eigenvalue multipliers
        return e_next, H_t, J_t, lambdas_t, m_t

    def run(self, T: int) -> t.Dict[str, t.List[Array]]:
        """
        Run T steps and log e_t, H_t, J_t, lambda_t, m_t.
        Returns a dict with keys: 'e', 'H', 'J', 'lambdas', 'm'.
        The list 'e' has T+1 entries: e[0] = e0, e[t] = e_t.
        also returns clip count (check validity of stability window)
        """
        e_hist: t.List[Array] = [self.e0.copy()]
        H_hist: t.List[Array] = []
        J_hist: t.List[Array] = []
        lam_hist: t.List[Array] = []
        m_hist: t.List[Array] = []

        e_t = self.e0.copy()
        for t in range(T):
            e_t, H_t, J_t, lambdas_t, m_t = self.step(t, e_t)
            e_hist.append(e_t)
            H_hist.append(H_t)
            J_hist.append(J_t)
            lam_hist.append(lambdas_t)
            m_hist.append(m_t)

        # final stats summary
        print(f"\nRun finished: {self.clip_count} clipping event(s) occurred.")
        return {
            "e": e_hist,
            "H": H_hist,
            "J": J_hist,
            "lambdas": lam_hist,
            "m": m_hist,
            "clip_count": self.clip_count,
            "clip_log": self.clip_log,
        }

@dataclasses.dataclass
class BWQREstimator:
    """
    Benettin–Wolf QR online Lyapunov estimator with positive–diagonal R.
    This is step by step what various implementations have done, I wrote
    some things slightly differently and so it is my own, but I don't
    claim complete originality over this work, as I'm sure is appropriate
    given it is a classical method but I didn't want to reinvent the wheel
    here
    """
    P: int
    Q0: t.Optional[Array] = None

    def __post_init__(self) -> None:
        if self.Q0 is None:
            self.Q = numpy.eye(self.P)
        else:
            assert self.Q0.shape == (self.P, self.P)
            self.Q = self.Q0.copy()
        self.logs: t.List[Array] = []
        # each entry is log diag(R_t), shape (P,)

    @staticmethod
    def _qr_positive_diagonal(Y: Array) -> t.Tuple[Array, Array]:
        Q, R = numpy.linalg.qr(Y)
        #standard, computes Y = QR
        #Q orthogonal, R upper triangular
        d = numpy.sign(numpy.diag(R))
        #diagonal sign extractor
        d[d == 0.0] = 1.0
        #degenerate handler
        D = numpy.diag(d)
        #diag matrix former
        Q = Q @ D
        R = D @ R
        #(QD)(DR) = QR
        return Q, R

    def update(self, J_t: Array) -> None:
        """
        Ingest one linear factor J_t and update QR logs.
        """
        Y = J_t @ self.Q
        Q, R = self._qr_positive_diagonal(Y)
        self.Q = Q
        self.logs.append(numpy.log(numpy.diag(R)))

    def spectrum(self) -> Array:
        """
        Return current estimates (time-averaged logs of R-diagonal).
        """
        if not self.logs:
            return numpy.zeros(self.P)
        L = numpy.stack(self.logs, axis=0)
        #shape (T, P)
        return L.mean(axis=0)

#spectrum comparison helpers
def compare_sorted(bw: Array, exact: Array) -> None:
    """Order-agnostic check: compare spectra as multisets (sorted)."""
    bw_s = numpy.sort(bw)
    ex_s = numpy.sort(exact)
    l2 = float(numpy.linalg.norm(bw_s - ex_s))
    print("\n[Multiset check] sorted BW   :", bw_s)
    print("[Multiset check] sorted exact:", ex_s)
    print(f"[Multiset check] L2 diff (sorted) = {l2:.6e}")

def align_by_permutation(bw: Array, exact: Array) -> t.Tuple[Array, Array]:
    """
    Align 'exact' to 'bw' by the best one-to-one pairing.
    Returns (exact_aligned, perm), where perm[j] is the index in 'exact'
    paired to bw[j]. Uses Hungarian if available; else greedy fallback.
    Purpose
    -------
    Basically, lyapunov exponents get output, but in a funky order so they
    need to be matched manually, this is not incorrect to do this. Chat-GPT
    assisted me with this particular code heavily, I wanted to disclose that
    properly
    """
    try:
        from scipy.optimize import linear_sum_assignment  # type: ignore
        cost = (bw[:, None] - exact[None, :]) ** 2
        ri, cj = linear_sum_assignment(cost)
        exact_aligned = exact[cj]
        return exact_aligned, cj
    except Exception:
        # Greedy nearest-neighbor (ok when exponents are well-separated)
        remaining = list(range(len(exact)))
        perm = []
        for x in bw:
            j_best = min(remaining, key=lambda k: (x - exact[k]) ** 2)
            perm.append(j_best)
            remaining.remove(j_best)
        perm = numpy.asarray(perm, dtype=int)
        return exact[perm], perm

def expected_log_m_for_eta(model: CommutingQuadraticClass,
                           eta: float,
                           trials: int = 20000) -> numpy.ndarray:
    """
    Monte Carlo approximation of the full Lyapunov spectrum
    for constant step size eta in the commuting quadratic class.
    Returns an array of shape (P,) whose i-th entry is
        E[ log |1 - eta * lambda_i(xi)| ].
    """
    P = model.P
    acc = numpy.zeros(P, dtype=float)
    for _ in range(trials):
        #shape (P,)
        lam = model.sample_lambdas()
        #shape (P,)
        m = 1.0 - eta * lam
        #accumulate log|m_i
        acc += numpy.log(numpy.abs(m))
    return acc / float(trials)

if __name__ == "__main__":
    rng = numpy.random.default_rng(7)
    P = 6

    #Shared eigenbasis U
    U = orthogonal_matrix_from_qr(P, rng)

    #Flat mask Z (two flat directions: i=0,1) and curvature stats for others
    Z = numpy.array([True, True, False, False, False, False])
    means = numpy.array([
        0.0,  # flat 0
        0.0,  # flat 1
        0.80, # curved 2 (smallest curved)
        0.95, # curved 3
        1.05, # curved 4
        1.15, # curved 5 (largest curvature)
    ])

    stds = numpy.array([
        0.0,  # flat 0
        0.0,  # flat 1
        0.04, # curved 2
        0.04, # curved 3
        0.04, # curved 4
        0.04, # curved 5
    ])

    #target sup of eta lambda (strictly < 2)
    MAX_STAB_PRODUCT = 1.9
    # maximum step size
    ETABAR = 1.4
    # ~1.357
    LAMBDA_MAX = MAX_STAB_PRODUCT / ETABAR

    #xi_sampler: returns one lambda(xi) ∈ R^P (nonnegative; Z will be zeroed again in model)
    #only randomness inside of the algorithm -> makes H random
    def xi_sampler() -> numpy.ndarray:
        lambdas = rng.normal(loc=means, scale=stds)
        lambdas[:2] = 0.0
        #enforce upper-bound on lambdas
        #enforce exact flats (belt-and-suspenders; model also enforces)
        lambdas = numpy.clip(lambdas, 0.0, LAMBDA_MAX)
        return lambdas

    #Build commuting quadratic model
    model = CommutingQuadraticClass(P=P, U=U, xi_sampler=xi_sampler, Z=Z)
    #enforce overparamaterization for Z
    model.validate_flats()

    #Set w* and initial error e0
    w_star = numpy.zeros(P)
    e0 = rng.normal(size=P)

    #Choose stepsize where we try to get eta·lambda_max(xi) < 2.
    eta = 1.4

    #addition of direct monte carlo evaluation of the theoretical
    #spectrum, should help to understand what is actually going on
    exact_lyap = expected_log_m_for_eta(model, eta=eta, trials=50000)

    #Run SGD for T steps (with stability enforcement + clipping counter)
    sgd = SGDDynamics(model=model, w_star=w_star, e0=e0, eta=eta, enforce_stability=True)
    T = 2000
    logs = sgd.run(T)  # returns dict with 'e','H','J','lambdas','m','clip_count','clip_log'

    #Benettin–Wolf QR (align Q0=U for per-step identities in this class)
    bw = BWQREstimator(P=P, Q0=U)
    for J_t in logs["J"]:
        bw.update(J_t)
    est_lyap = bw.spectrum()

    #Diagnostics / prints
    print("\n=== Benettin–Wolf QR Estimates ===")
    for i, val in enumerate(est_lyap):
        print(f"lambda_hat[{i}] = {val:.6f}")
    print(f"\nClipping events: {logs['clip_count']}")
    if logs["clip_count"] > 0:
        print("First few clips (step, eta_original, scale):", logs["clip_log"][:5])

    #Compare to time-average of log|m_t| from dynamics (should match in this class)
    #Optional not always enforced
    m_stack = numpy.stack(logs["m"], axis=0)  # shape (T, P)
    empirical_lyap = numpy.mean(numpy.log(numpy.abs(m_stack)), axis=0)
    print("\n=== Cross-check: mean log|m_t| (empirical) ===")
    for i, val in enumerate(empirical_lyap):
        print(f"lambda_emp[{i}] = {val:.6f}")

    #Normal distance to manifold vs time
    dists = [model.normal_distance(e) for e in logs["e"]]
    print(f"\nd_0 = {dists[0]:.3e}, d_T = {dists[-1]:.3e}")

    #Plots
    T_vis = 400
    t_vis = numpy.arange(T_vis)
    #Distance to manifold over time (linear and log scale)
    fig1 = plt.figure(figsize=(7, 4))
    plt.plot(t_vis, dists[:T_vis])
    plt.xlabel(r"$t$")
    plt.ylabel(r"$\|P_{\perp} e_t\|$")
    plt.title("Normal distance to manifold vs. time (early iterations)")
    plt.tight_layout()

    fig2 = plt.figure(figsize=(7, 4))
    #avoid log(0): add tiny epsilon
    eps = 1e-16
    log_dists = numpy.log(numpy.array(dists) + eps)
    plt.plot(t_vis, log_dists[:T_vis])
    plt.xlabel(r"$t$")
    plt.ylabel(r"$\log\|P_{\perp} e_t\|$")
    plt.title("Log normal distance to manifold vs. time (early iterations)")
    plt.tight_layout()

    #Lyapunov spectrum comparison: theory vs BW–QR vs time-average
    #Multiset (order-agnostic) check
    compare_sorted(est_lyap, empirical_lyap)

    #Per-index alignment via greedy: align empirical_lyap to est_lyap
    emp_aligned, perm = align_by_permutation(est_lyap, empirical_lyap)
    l2_aligned = float(numpy.linalg.norm(est_lyap - emp_aligned))
    print("\n[Per-index alignment] perm (exact idx -> BW order):", perm)
    print(f"[Per-index alignment] L2 diff (aligned) = {l2_aligned:.6e}")

    #Now consider the *spectrum*: sort each method's exponents in decreasing order
    #so that k = 1 is the largest exponent, k = 2 the second largest, etc.
    est_sorted   = numpy.sort(est_lyap)[::-1]
    emp_sorted   = numpy.sort(emp_aligned)[::-1]
    exact_sorted = numpy.sort(exact_lyap)[::-1]

    #exponent index (1 = largest)
    k = numpy.arange(1, P + 1)

    fig3 = plt.figure(figsize=(7, 4))
    plt.plot(k, exact_sorted, "o-", label=r"MC $\mathbb{E}[\log|1 - \eta \lambda_i(\xi)|]$")
    plt.plot(k, est_sorted,   "x--", label="BW–QR")
    plt.plot(k, emp_sorted,   "s:", label=r"time-avg $\log|m_{t,i}|$")

    plt.xlabel(r"Exponent index $k$ (sorted)")
    plt.ylabel(r"Lyapunov exponent $\lambda_k$")
    plt.title("Lyapunov spectrum: theory vs BW–QR vs time-average")
    plt.legend()
    plt.tight_layout()

    plt.show()

from typing import Callable, Dict
#stepsize schedules

def const_eta(eta: float) -> Callable[[int], float]:
    return lambda t: float(eta)

def piecewise_constant_eta(breaks, values) -> Callable[[int], float]:
    # breaks: strictly increasing ints, values: len(breaks)+1
    def _eta(t: int) -> float:
        for b, v in zip(breaks, values):
            if t < b:
                return float(v)
        return float(values[-1])
    return _eta

def cosine_decay_eta(eta_max: float, eta_min: float, T_max: int) -> Callable[[int], float]:
    assert eta_max >= eta_min > 0.0 and T_max >= 1
    def _eta(t: int) -> float:
        if t >= T_max:
            return float(eta_min)
        cos_term = 0.5 * (1 + math.cos(math.pi * t / T_max))
        return float(eta_min + (eta_max - eta_min) * cos_term)
    return _eta

def exp_decay_eta(eta0: float, gamma: float) -> Callable[[int], float]:
    # eta_t = eta0 * gamma^t, 0<gamma<=1
    assert eta0 > 0.0 and 0.0 < gamma <= 1.0
    return lambda t: float(eta0 * (gamma ** t))

#Closed form (Monte Carlo) exponents for constant or scheduled step sizes
#estimates expected logarithmic contraction rates for a constant step size
def expected_log_m_for_eta(model, eta: float, trials: int = 20000) -> Array:
    P = model.P
    #retrive model dim P
    acc = numpy.zeros(P, dtype=float)
    #init accumulator to hold stuff
    for _ in range(trials):
        lam = model.sample_lambdas()
        m = 1.0 - eta * lam
        acc += numpy.log(numpy.abs(m))
    return acc / float(trials)

def expected_log_m_for_schedule(model, eta_schedule, T_bar: int = 200, trials_per_t: int = 5000) -> Array:
    P = model.P
    acc = numpy.zeros(P, dtype=float)
    for t in range(T_bar):
        eta_t = float(eta_schedule(t))
        #retrive learning rate for this iter
        local = numpy.zeros(P, dtype=float)
        #hold monte carlo loop size (P,)
        for _ in range(trials_per_t):
            lam = model.sample_lambdas()
            m = 1.0 - eta_t * lam
            local += numpy.log(numpy.abs(m))
            #same as before using time dependent LR
        acc += local / float(trials_per_t)
    return acc / float(T_bar)
    #return average over all time steps

def estimate_lambda_max(model, draws: int = 200000) -> float:
    lam_max = 0.0
    for _ in range(draws):
        lam = model.sample_lambdas()
        lam_max = max(lam_max, float(lam.max()))
    return lam_max

def tune_eta_to_edge(model,
                     target: float = -1e-3,
                     draws_for_bound: int = 100000,
                     trials_per_eta: int = 30000,
                     tol: float = 5e-4,
                     max_iter: int = 50) -> float:
    """
    Solve for eta so that E[log|1 - eta * lambda_top(xi)|] ~= target (target<0 small).
    Returns eta_star in (0, 2/lam_max_est).
    """
    assert target < 0.0
    lam_max_est = estimate_lambda_max(model, draws=draws_for_bound)
    if lam_max_est <= 0.0:
        return 0.0

    eta_lo = 0.0
    eta_hi = 1.999 / lam_max_est
    #strictly inside stability window

    def eval_expected_top(eta: float) -> float:
      #defines helper func to evaluate the expected top LE
      #bisection search
        vals = []
        for _ in range(trials_per_eta):
          #loop over random draws
            lam = model.sample_lambdas()
            vals.append(numpy.log(numpy.abs(1.0 - eta * float(lam.max()))))
            #extract the largest curvature from that sample (trying to match target)
        return float(numpy.mean(vals))
        #return monte carlo mean

    f_hi = eval_expected_top(eta_hi)
    #evalute top lyapunov expoonent at the high--end boundary
    if f_hi > target:
        return eta_hi
        #best we can do with this bound

    for _ in range(max_iter):
        eta_mid = 0.5 * (eta_lo + eta_hi)
        #compute the midpoint eta between the current lower and upper bounds
        f_mid = eval_expected_top(eta_mid)
        #evaluate thje expected top exponent
        if f_mid > target:
            eta_lo = eta_mid
        else:
            eta_hi = eta_mid
            #bisection logic
        if abs(f_mid - target) <= tol:
            return eta_mid
    return 0.5 * (eta_lo + eta_hi)

#schedule_comparison_driver
def schedule_comparison_driver(model,
                               w_star: Array,
                               e0: Array,
                               base_eta: float,
                               T: int = 2000) -> None:

    schedules: Dict[str, Callable[[int], float]] = {
        f"const({base_eta:.3f})": const_eta(base_eta),
        "piecewise([600,1300]; [1.0, 0.8, 0.6] * eta)": piecewise_constant_eta(
            breaks=[600, 1300],
            values=[1.0*base_eta, 0.8*base_eta, 0.6*base_eta],
        ),
        "cosine(eta->0.5eta, T_max=1000)": cosine_decay_eta(
            eta_max=base_eta, eta_min=0.5*base_eta, T_max=1000
        ),
        "exp(gamma=0.999)": exp_decay_eta(base_eta, gamma=0.999),
    }

    T_vis = 400
    t_vis = numpy.arange(T_vis)

    # Plot 1: log normal distance for all schedules
    plt.figure(figsize=(8, 4.5))
    for name, sched in schedules.items():
        sgd_sched = SGDDynamics(model=model, w_star=w_star, e0=e0,
                                eta_schedule=sched, enforce_stability=True)
        logs_sched = sgd_sched.run(T)
        dists = [model.normal_distance(e) for e in logs_sched["e"]]

        dists_arr = numpy.array(dists)
        log_dists = numpy.log(dists_arr + 1e-16)

        # show first T_vis steps (here T_vis = 2000)
        plt.plot(t_vis, log_dists[:T_vis], label=name)

    plt.xlabel(r"$t$")
    plt.ylabel(r"$\log\|P_{\perp} e_t\|$")
    plt.title("Log normal distance to manifold vs. time (schedules)")
    plt.legend()
    plt.tight_layout()

    #Plot 2 (1+N): spectra per schedule (BW–QR vs exact)
    for idx, (name, sched) in enumerate(schedules.items(), start=1):
        sgd_sched = SGDDynamics(model=model, w_star=w_star, e0=e0, eta_schedule=sched, enforce_stability=True)
        logs_sched = sgd_sched.run(T)
        bw = BWQREstimator(P=model.P, Q0=model.U)
        for J_t in logs_sched["J"]:
            bw.update(J_t)
        est_lyap = bw.spectrum()

        #Exact (Monte Carlo) Cesaro
        exact_lyap = expected_log_m_for_schedule(model, sched, T_bar=200, trials_per_t=5000)

        print(f"\n[{name}] BW–QR vs exact exponents:")
        for i, (a, b) in enumerate(zip(est_lyap, exact_lyap)):
            print(f"  i={i}: bw={a:.6f}, exact={b:.6f}")

        plt.figure(figsize=(7.5, 4))
        ixs = numpy.arange(model.P)
        plt.bar(ixs - 0.15, est_lyap, width=0.3, label="BW–QR")
        plt.bar(ixs + 0.15, exact_lyap, width=0.3, label="Exact E[log|m_t|]")
        plt.xlabel("Index i")
        plt.ylabel("Lyapunov exponent")
        plt.title(f"Spectrum comparison: {name}")
        plt.legend()
        plt.tight_layout()

    plt.show()

#edge-tuned schedules driver
def edge_tuned_schedules_driver(model,
                                w_star: Array,
                                e0: Array,
                                target: float = -1e-3,
                                T: int = 2000) -> None:

    eta_star = tune_eta_to_edge(model, target=target)
    print(f"\n[Edge tuner] eta_star ~= {eta_star:.6f} for target {target:.1e}")

    tuned_schedules: Dict[str, Callable[[int], float]] = {
        f"const(eta*)": const_eta(eta_star),
        "cosine(eta*->0.5eta*, T_max=1000)": cosine_decay_eta(
            eta_max=eta_star, eta_min=0.5*eta_star, T_max=1000
        ),
        "piecewise([600,1300]; [1.0,0.85,0.7]*eta*)": piecewise_constant_eta(
            breaks=[600, 1300],
            values=[1.0*eta_star, 0.85*eta_star, 0.7*eta_star],
        ),
        "exp(gamma=0.9995)": exp_decay_eta(eta_star, gamma=0.9995),
    }

    #Plot 1: log normal distance for all edge-tuned schedules
    plt.figure(figsize=(8, 4.5))
    for name, sched in tuned_schedules.items():
        sgd_sched = SGDDynamics(model=model, w_star=w_star, e0=e0, eta_schedule=sched, enforce_stability=True)
        logs_sched = sgd_sched.run(T)
        dists = [model.normal_distance(e) for e in logs_sched["e"]]
        plt.plot(numpy.log(numpy.array(dists) + 1e-16), label=name)
    plt.xlabel("t")
    plt.ylabel("log ||P_perp e_t||")
    plt.title("Log normal distance to manifold vs. time (edge-tuned schedules)")
    plt.legend()
    plt.tight_layout()

    #Per-schedule spectra: BW–QR vs TRUE exponents
    for name, sched in tuned_schedules.items():
        sgd_sched = SGDDynamics(model=model, w_star=w_star, e0=e0, eta_schedule=sched, enforce_stability=True)
        logs_sched = sgd_sched.run(T)

        bw = BWQREstimator(P=model.P, Q0=model.U)
        for J_t in logs_sched["J"]:
            bw.update(J_t)
        est_lyap = bw.spectrum()

        #TRUE exponents (check math in the document):
        #constant eta*:   E[log|1 - eta* lambda_i|]
        #schedule:        Cesaro avg 1/T * sum E[log|1 - eta_t lambda_i|]
        if "const" in name:
            true_lyap = expected_log_m_for_eta(model, eta_star, trials=40000)
        else:
            true_lyap = expected_log_m_for_schedule(model, sched, T_bar=200, trials_per_t=5000)

        print(f"\n[Edge tuned: {name}] BW–QR vs TRUE:")
        for i, (a, b) in enumerate(zip(est_lyap, true_lyap)):
            print(f"  i={i}: bw={a:.6f}, true={b:.6f}")

        plt.figure(figsize=(7.5, 4))
        ixs = numpy.arange(model.P)
        plt.bar(ixs - 0.15, est_lyap, width=0.3, label="BW–QR")
        plt.bar(ixs + 0.15, true_lyap, width=0.3, label="True (expectation)")
        plt.xlabel("Index i")
        plt.ylabel("Lyapunov exponent")
        plt.title(f"Edge-tuned spectrum: {name}")
        plt.legend()
        plt.tight_layout()

    plt.show()

if __name__ == "__main__":
    rng = numpy.random.default_rng(7)
    P = 6
    U = orthogonal_matrix_from_qr(P, rng)

    Z = numpy.array([True, True, False, False, False, False])
    #this choice is extremely specific for the means and stds
    #ostensibly we are trying to make eta lambda live in the branch
    #with high probability for the curved modes we actually care about
    #fits well with the theory and toy model and is easily generalized
    #also the clipping will almost never destroy the top eigenvalue,
    #so this is totally kosher
    means = numpy.array([
        0.0,  # flat 0
        0.0,  # flat 1
        0.80, # curved 2 (smallest curved)
        0.95, # curved 3
        1.05, # curved 4
        1.15, # curved 5 (largest curvature)
    ])

    stds = numpy.array([
        0.0,  # flat 0
        0.0,  # flat 1
        0.04, # curved 2
        0.04, # curved 3
        0.04, # curved 4
        0.04, # curved 5
    ])

    MAX_STAB_PRODUCT = 1.9
    #same buffer
    ETABAR = 1.4
    #base_eta is the largest schedule value
    LAMBDA_MAX = MAX_STAB_PRODUCT / ETABAR

    def xi_sampler() -> numpy.ndarray:
        lam = rng.normal(loc=means, scale=stds)
        lam[:2] = 0.0
        lam = numpy.clip(lam, 0.0, LAMBDA_MAX)
        return lam

    model = CommutingQuadraticClass(P=P, U=U, xi_sampler=xi_sampler, Z=Z)
    #enforce overparamaterization for Z
    model.validate_flats()
    w_star = numpy.zeros(P)
    e0 = rng.normal(size=P)

    #Compare raw schedules
    schedule_comparison_driver(model, w_star, e0, base_eta=1.4, T=2000)

    #Edge-tuned schedules (dominant ~ 0-)
    edge_tuned_schedules_driver(model, w_star, e0, target=-1e-3, T=2000)